/**
 * @file KnowledgeStress.test.ts - Stress tests for knowledge service
 * @description Tests extreme/edge cases: memory exhaustion (OOM), infinite processing, system hangs. Scenarios: huge text files, deeply nested structures, malformed input, concurrent processing. Uses MOCKED streams - does not create actual large files.
 * @depends Knowledge service chunking logic
 */

import { Readable } from 'stream';
import { afterEach, beforeEach, describe, expect, it, vi } from 'vitest';

// ====== Mock Setup ======

// Mock fs-extra to control file reading behavior
vi.mock('fs-extra', () => ({
  default: {
    readFile: vi.fn(),
    pathExists: vi.fn().mockResolvedValue(true),
    stat: vi.fn(),
    ensureDir: vi.fn().mockResolvedValue(undefined),
    createReadStream: vi.fn(),
  },
  readFile: vi.fn(),
  pathExists: vi.fn().mockResolvedValue(true),
  stat: vi.fn(),
  ensureDir: vi.fn().mockResolvedValue(undefined),
  createReadStream: vi.fn(),
}));

// ====== Test Utilities ======

/**
 * Create a mock readable stream that emits data chunks
 */
function createMockStream(totalSize: number, chunkSize: number = 64 * 1024, delayMs = 0): Readable {
  let bytesEmitted = 0;

  return new Readable({
    read() {
      // Stop if we've emitted enough data
      if (bytesEmitted >= totalSize) {
        this.push(null);
        return;
      }

      const remaining = totalSize - bytesEmitted;
      const currentChunkSize = Math.min(chunkSize, remaining);

      const chunk = Buffer.alloc(currentChunkSize, 'x');
      bytesEmitted += currentChunkSize;

      if (delayMs > 0) {
        setTimeout(() => this.push(chunk), delayMs);
      } else {
        this.push(chunk);
      }
    },
  });
}

/**
 * Create a mock stream that never ends (for timeout testing)
 */
function createInfiniteStream(chunkSize = 1024): Readable {
  return new Readable({
    read() {
      const chunk = Buffer.alloc(chunkSize, 'x');
      this.push(chunk);
    },
  });
}

/**
 * Create a mock stream that throws an error after N bytes
 */
function createFailingStream(failAfterBytes: number): Readable {
  let bytesEmitted = 0;

  return new Readable({
    read() {
      if (bytesEmitted >= failAfterBytes) {
        this.destroy(new Error('Simulated stream error'));
        return;
      }

      const chunk = Buffer.alloc(1024, 'x');
      bytesEmitted += 1024;
      this.push(chunk);
    },
  });
}

/**
 * Track memory usage during a function execution
 */
async function trackMemoryUsage<T>(
  fn: () => Promise<T>
): Promise<{ result: T; peakMemoryMB: number; startMemoryMB: number; endMemoryMB: number }> {
  // Force GC if available (run with --expose-gc)
  if (global.gc) {
    global.gc();
  }

  const startMemory = process.memoryUsage().heapUsed;
  let peakMemory = startMemory;

  // Sample memory every 10ms
  const memoryInterval = setInterval(() => {
    const current = process.memoryUsage().heapUsed;
    if (current > peakMemory) {
      peakMemory = current;
    }
  }, 10);

  try {
    const result = await fn();
    const endMemory = process.memoryUsage().heapUsed;

    return {
      result,
      peakMemoryMB: peakMemory / 1024 / 1024,
      startMemoryMB: startMemory / 1024 / 1024,
      endMemoryMB: endMemory / 1024 / 1024,
    };
  } finally {
    clearInterval(memoryInterval);
  }
}

// ====== Text Chunking Logic (Extracted for Testing) ======

/**
 * Simple text chunking implementation
 */
function chunkTextWithOverlap(content: string, chunkSize = 1000, overlap = 200): string[] {
  const chunks: string[] = [];
  let start = 0;

  while (start < content.length) {
    const end = Math.min(start + chunkSize, content.length);
    chunks.push(content.slice(start, end));

    start += chunkSize - overlap;

    if (start === end && start < content.length) {
      start = end;
    }
  }

  return chunks;
}

/**
 * Stream-based text chunking (memory efficient)
 */
async function chunkTextStream(
  stream: Readable,
  chunkSize,
  overlap,
  onChunk: (chunk: string, index: number) => void,
  options?: { maxChunks?: number; maxBytes?: number; timeout?: number }
): Promise<{ totalChunks: number; totalBytes: number; truncated: boolean }> {
  const maxChunks = options?.maxChunks ?? Number.POSITIVE_INFINITY;
  const maxBytes = options?.maxBytes ?? Number.POSITIVE_INFINITY;
  const timeout = options?.timeout ?? 60000;

  return new Promise((resolve, reject) => {
    let buffer = '';
    let chunkIndex = 0;
    let totalBytes = 0;
    let truncated = false;
    let timeoutHandle: NodeJS.Timeout | null = null;

    if (timeout > 0) {
      timeoutHandle = setTimeout(() => {
        truncated = true;
        stream.destroy();
        resolve({ totalChunks: chunkIndex, totalBytes, truncated: true });
      }, timeout);
    }

    const cleanup = () => {
      if (timeoutHandle) {
        clearTimeout(timeoutHandle);
      }
    };

    stream.on('data', (data: Buffer) => {
      totalBytes += data.length;

      if (totalBytes > maxBytes) {
        truncated = true;
        stream.destroy();
        return;
      }

      buffer += data.toString('utf-8');

      while (buffer.length >= chunkSize && chunkIndex < maxChunks) {
        const chunk = buffer.slice(0, chunkSize);
        onChunk(chunk, chunkIndex);
        chunkIndex++;

        buffer = buffer.slice(chunkSize - overlap);
      }

      if (chunkIndex >= maxChunks) {
        truncated = true;
        stream.destroy();
      }
    });

    stream.on('end', () => {
      cleanup();

      if (buffer.length > 0 && chunkIndex < maxChunks) {
        onChunk(buffer, chunkIndex);
        chunkIndex++;
      }

      resolve({ totalChunks: chunkIndex, totalBytes, truncated });
    });

    stream.on('error', (err) => {
      cleanup();
      reject(err);
    });
  });
}

// ====== Fast Unit Tests (Always Run) ======

describe('Knowledge Service - Text Chunking (Fast)', () => {
  describe('chunkTextWithOverlap', () => {
    it('should correctly chunk with overlap', () => {
      const text = 'A'.repeat(100) + 'B'.repeat(100) + 'C'.repeat(100);
      const chunks = chunkTextWithOverlap(text, 100, 20);

      expect(chunks[0]).toBe('A'.repeat(100));
      expect(chunks[1].startsWith('A'.repeat(20))).toBe(true);
      expect(chunks.length).toBe(4);
    });

    it('should handle empty input', () => {
      const chunks = chunkTextWithOverlap('', 1000, 200);
      expect(chunks).toEqual([]);
    });

    it('should handle input smaller than chunk size', () => {
      const text = 'Hello, World!';
      const chunks = chunkTextWithOverlap(text, 1000, 200);

      expect(chunks.length).toBe(1);
      expect(chunks[0]).toBe(text);
    });

    it('should handle zero overlap', () => {
      const text = 'A'.repeat(100) + 'B'.repeat(100);
      const chunks = chunkTextWithOverlap(text, 100, 0);

      expect(chunks.length).toBe(2);
      expect(chunks[0]).toBe('A'.repeat(100));
      expect(chunks[1]).toBe('B'.repeat(100));
    });

    it('should handle Unicode correctly', () => {
      const text = 'ä½ å¥½ä¸–ç•Œ'.repeat(100);
      const chunks = chunkTextWithOverlap(text, 50, 10);

      for (const chunk of chunks) {
        expect(chunk).toMatch(/^[ä½ å¥½ä¸–ç•Œ]+$/);
      }
    });

    it('should handle special characters', () => {
      const text = `<script>alert("xss")</script>\n\0\r\n\t${'ðŸ˜€'.repeat(50)}`;
      const chunks = chunkTextWithOverlap(text, 100, 20);

      expect(chunks.length).toBeGreaterThan(0);
    });
  });
});

// ====== Slow Stress Tests (Skip by Default) ======
// These stress tests involve stream processing and may be slow in some environments
// To run full stress tests, use: npm run test:stress
describe.skip('Knowledge Service - Stress Tests (SLOW - Run manually)', () => {
  beforeEach(() => {
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.restoreAllMocks();
  });

  describe('Huge Text File Handling', () => {
    it('should handle large text without OOM using streaming', async () => {
      const FILE_SIZE = 10 * 1024; // 10KB
      const CHUNK_SIZE = 500;
      const OVERLAP = 100;

      let chunkCount = 0;
      const stream = createMockStream(FILE_SIZE, 2 * 1024);

      const result = await chunkTextStream(stream, CHUNK_SIZE, OVERLAP, () => {
        chunkCount++;
      });

      expect(result.truncated).toBe(false);
      expect(result.totalBytes).toBe(FILE_SIZE);
      expect(result.totalChunks).toBeGreaterThan(0);
      expect(chunkCount).toBe(result.totalChunks);
    });

    it('should enforce chunk limit for huge files', async () => {
      const FILE_SIZE = 20 * 1024; // 20KB
      const MAX_CHUNKS = 10; // Limit to 10 chunks

      const stream = createMockStream(FILE_SIZE, 2 * 1024);
      let processedChunks = 0;

      const result = await chunkTextStream(
        stream,
        500,
        100,
        () => {
          processedChunks++;
        },
        { maxChunks: MAX_CHUNKS }
      );

      expect(result.truncated).toBe(true);
      expect(result.totalChunks).toBeLessThanOrEqual(MAX_CHUNKS);
      expect(processedChunks).toBeLessThanOrEqual(MAX_CHUNKS);
    });

    it('should enforce byte limit for huge files', async () => {
      const MAX_BYTES = 5 * 1024; // 5KB limit
      const stream = createMockStream(20 * 1024, 2 * 1024); // 20KB file

      const result = await chunkTextStream(stream, 500, 100, () => {}, { maxBytes: MAX_BYTES });

      expect(result.truncated).toBe(true);
      expect(result.totalBytes).toBeLessThanOrEqual(MAX_BYTES + 4 * 1024);
    });

    it('should timeout on infinite streams', async () => {
      const infiniteStream = createInfiniteStream(512);
      const TIMEOUT = 50;

      const result = await chunkTextStream(infiniteStream, 500, 100, () => {}, {
        timeout: TIMEOUT,
      });

      expect(result.truncated).toBe(true);
      expect(result.totalBytes).toBeGreaterThan(0);
    });

    it('should handle stream errors gracefully', async () => {
      const failingStream = createFailingStream(50 * 1024); // Fail after 50KB

      await expect(chunkTextStream(failingStream, 1000, 200, () => {})).rejects.toThrow(
        'Simulated stream error'
      );
    });
  });

  // Text chunking tests moved to fast test suite above

  describe('Concurrent Processing', () => {
    it('should handle multiple simultaneous processing requests', async () => {
      const files = Array.from({ length: 3 }, (_, i) => ({
        stream: createMockStream(5 * 1024, 1024), // 5KB each
        id: `file-${i}`,
      }));

      const results = await Promise.all(
        files.map(async (file) => {
          let chunks = 0;
          await chunkTextStream(file.stream, 500, 100, () => {
            chunks++;
          });
          return { id: file.id, chunks };
        })
      );

      expect(results).toHaveLength(3);
      for (const result of results) {
        expect(result.chunks).toBeGreaterThan(0);
      }
    });
  });

  // Edge case tests moved to fast test suite above

  describe('Memory Efficiency (Integration)', () => {
    it('should process large content without loading entirely into memory', async () => {
      const STREAM_SIZE = 10 * 1024;
      const stream = createMockStream(STREAM_SIZE, 2 * 1024);

      let maxChunkSize = 0;
      let totalChunks = 0;

      await chunkTextStream(stream, 500, 100, (chunk) => {
        totalChunks++;
        if (chunk.length > maxChunkSize) {
          maxChunkSize = chunk.length;
        }
      });

      expect(maxChunkSize).toBeLessThanOrEqual(500);
      expect(totalChunks).toBeGreaterThan(0);
    });
  });
});

// ====== Export for other tests ======

export {
  createMockStream,
  createInfiniteStream,
  createFailingStream,
  chunkTextWithOverlap,
  chunkTextStream,
  trackMemoryUsage,
};
